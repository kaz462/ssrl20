---
title: "First Task - Sampling Prep"
date: "May 24, 2020"
output:
  html_document:
    df_print: paged
    toc: yes
  md_document:
    df_print: paged
    toc: yes
    variant: markdown_github
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```


# Preparation
```{r}
library(httr)
library(tidyverse)  
library(rtweet)
### app name from api set-up
# Access token : 1263179887793192960-uyUL04poBmUkKApGrtZeioFgys5ChO
# Access token secret :G6MBK3C7mm5Yp0UXAHTLFRBmZAW0CLNDRwrPanVZFABTZ
# appname <- "OnlineTeaching"
# key <- "JY9cHldmwIijQMUcfJXMzvWRT"
# secret <- "S9ZC9F62Dcp4W1u1aBp9izxaLNarQ1rzQyqIqEyH0kmOkV1ruA"
### create token named "twitter_token"
# twitter_token <- rtweet::create_token(app = appname,
#                                       consumer_key = key,
#                                       consumer_secret = secret)
```

# Package rtweet
## Is the rtweet package the best option to live stream tweets?  
There are several R packages for interacting with Twitter’s APIs. "rtweet" is the only one that can access both Twitter’s REST and stream APIs.The following link provides detailed comparisons of Package Functionality with other packages:
https://rtweet.info/

## Do we need a subscription to be able to download and save the tweets?
We do not need a subscription to be able to download and save the tweets. However, _**search_tweets()**_ _**ONLY RETURNS DATA FROM THE PAST 6-9 DAYS**_. We can get more access by applying Twitter premium APIs if needed, as introduced in the below link: https://developer.twitter.com/en/premium-apis.

## search_tweets( )
Returns Twitter statuses matching a user provided search query. 

### parameters to be specified (ONLY list the important ones)\
1. Query to be searched: Search for Twitter statuses containing a keyword, phrase, or multiple keywords. Basic search operators such as "and", "or" are used in the below examples. \

2. Specifying the total number of desired tweets to return. Defaults to 100. Maximum number of tweets returned from a single token is 18,000 - NO subscription needed for this number of tweets.

3. Character string specifying which type of search results to return from Twitter's REST API. The current default is type = "recent", other valid types include type = "mixed" and type = "popular".

4. include_rts: TRUE or FALSE, indicating whether to include retweets in search results. 

5. geocode: Geographical limiter of the template "latitude,longitude,radius". 

6. Specifying the language.

### Example
```{r}
## search for a keyword
rt <- search_tweets(q = "rstats", n = 5)
## search for a phrase
rt <- search_tweets(q = "data science", n = 5)
## search for multiple keywords
rt <- search_tweets(q = "rstats AND python", n = 5)
## search tweets (q = search query; n = desired number of tweets to return)
rt <- search_tweets(q = "rstats", n = 5)

## Use OR between search terms to find any match.
## search for any mention of a list of words
rt <- search_tweets("statistics OR statistical OR quantitative")
## Specify a language of the tweets and exclude retweets.

## search for tweets in english that are not retweets
rt <- search_tweets("rstats", lang = "en", include_rts = FALSE)
## Search by geo-location.
## search for tweets in english that are not retweets
## will get Google map API later if needed
#rt <- search_tweets("lang:en", geocode = lookup_coords("Chicago, IL"))
```





## stream_tweets( )
Returns public statuses via one of the following four methods:\
1. Sampling a small random sample of all publicly available tweets\
2. Filtering via a search-like query (up to 400 keywords)\
3. Tracking via vector of user ids (up to 5000 user_ids)\
4. Location via geo coordinates (1-360 degree location boxes)\
```{r}
## random sample
st <- stream_tweets(q = "", timeout = 1)
## keyword filter
st <- stream_tweets(q = "realDonaldTrump,Mueller", timeout = 1)
## geo locations
## will get Google map API later if needed
# st <- stream_tweets(q = lookup_coords("London, GB"), timeout = 1)
```


## Search API vs. Streaming API
* Past vs. Future\
This is the essential difference between the two APIs. Search goes back in time and streaming goes forward. When you first decide to collect tweets on a subject, you have nothing to start with. The search API is best used then to fill in the past 7 days for your subject matter. This is often called back-filling. With a database caught up to the present moment, you can then turn on the streaming API and capture all tweets going forward.
* Both need OAuth to connect to Twitter
* Rate limit rules are completely different
* Data formats are almost the same
* Search API has more powerful queries
* Seach API can collect a wider range of data
* Streaming API usually returns a much higher flow of tweets





# Export Data
## What data would we obtain? 
* Data inlcude both Tweets and Users' information. These objects all encapsulate core attributes that describe the object. 

* Each Tweet has an author, a message, a unique ID, a timestamp of when it was posted, and sometimes geo metadata shared by the user. 

* Each User has a Twitter name, an ID, a number of followers, and most often an account bio.



## Can we export the dataset in CSV files?
Yes. Function **_parse_stream_** in the rtweet package can convert Twitter stream data (JSON file) into parsed data frame. Both **_stream_tweets_** and **_search_tweets_** will automatically convert JSON file and return a data frame. 

## Demo (see attached csv files) 
* demo.csv contains 100 tweets with hashtag "#onlinelearning", where each observation (row) is a different tweet.
* user.csv contains corresponding users' information.
* Name of variables and their definition are documented in the below Tweet Data Dictionary: https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object

```{r}
demo <- search_tweets("#onlinelearning", lang = "en", include_rts = FALSE, n = 100)
## users data
user <- users_data(demo)
## Export to CSV
demo <- apply(demo, 2, as.character)
write.csv(demo, 'demo.csv')
write.csv(user, 'user.csv')
```






